---
notes for matlab deep learning
author: pyd
---

# deep learning using matlab

## 单层神经网络

1. 增量规则：

$$
\omega_{ij}=\omega_{ij}+\alpha e_{i} x_{j}
$$

​    其中$e_{i}​$ 为输出节点 $i​$ 的误差，$\alpha​$ 为学习率，$x_{j}​$ 为节点$j​$ 对节点$i​$ 的输入。$\omega_{ij}​$ 为节点$j​$ 到节点$i​$ 的权重。

2. 广义增量规则：
   $$
   \omega_{ij}=\omega_{ij}+\alpha \delta_{i} x_{j}
   $$
   其中$\delta_{i} = \phi' ( \nu_{i} ) e_{i}$ ,    $\nu_{i}$ 为节点$i$ 激活前的输出值。$\phi$ 为激活函数。

3. 权值更新原理：

   > 随机梯度下降法：随机选取每个数据，按照增量规则对网络参数进行权值调整。
   >
   > 批量算法：使用全部训练数据计算权重更新值，然后利用平均值来调整该权重。
   >
   > 小批量算法：每次使用部分数据的平均值。使训练过程分批进行。

4. 示例:

   > [随机梯度下降法](./mySGD.m "matlab文件")
   >
   > [批量算法](./myBatch.m  "matlab文件")
   >
   > [二者比较](./SGD_BATCH.m "matlab文件")

## 多层神经网络

1. 反向传播算法(BP)：解决隐藏层误差值的计算问题，以下以二输入，一层二隐藏，二输出为例

   * 输出节点的增量：计算方法与单层网络的广义增量规则相同。得到($\delta_{1},\delta_{2}$)。

   * 隐藏层节点的增量：只有 $e$ 的计算方法不同，是右侧节点的 $\delta$ 的加权和。
     $$
     e_{1}^{(1)}=\omega_{11}^{(2)} \delta_{1}+\omega_{21}^{(2)} \delta_{2}
     $$

     $$
     \delta_{1}^{(1)}=\phi'(v_{1}^{(1)}) e_{1}^{(1)}
     $$

     $\delta_{2}^{(1)}​$ 的计算同理。

   * 权重调整：与单层时情况相同。

2. 用BP算法解决XOR问题：

   基于问题的非线性，无法用单层网络解决，因此采用多层网络，网络结构为：三输入，一层四隐藏，一输出。

   [matlab代码](.\solveXOR.m "反向传播")

3. 动量法：调整权重的一种改进方法，令增量加上与之前的增量相关的项，使当前权重的更新受到之前所有权重更新值得影响
   $$
   \Delta \omega=\alpha \delta x
   $$

   $$
   m=\Delta \omega+\beta m^{-}
   $$

   $$
   \omega=\omega+m
   $$

   $$
   m^{-}=m
   $$

   [matlab代码](./solveXORMmt.m "动量法")

4. 代价函数：

   根据代价函数和激活函数的不同，推出不同的学习规则，即增量的计算公式不同。以下例子在Sigmoid激活下。

   > * 误差平方和函数：
   >   $$
   >   J=\sum_{i=1}^{M}\frac{1}{2}(d_{i}-y_{i})^{2}
   >   $$
   >   对应的学习规则为前面得增量公式。
   >
   > * 交叉熵函数：
   >   $$
   >   J=\sum_{i=1}^{M}[-d_{i} ln(y_{i})-(1-d_{i})ln(1-y_{i})]
   >   $$
   >   对应的学习规则：只有输出层的$\delta=e$ 不同，其他相同。
   >
   >   [交叉熵函数示例](./ce_examp.m "交叉熵函数")
   >
   > * [二者比较](./ce_sse.m "误差平方和和交叉熵的比较")

5. 问题：**具体是如何根据代价函数来得到学习规则的？**

## 神经网络分类

1. 二分类：一个输出节点即可，标签转化成0，1值后，使用Sigmoid函数，与前面所诉相同。
2. 多分类：

## 深度学习

## 卷积神经网络